"""Validation Agent - evaluates answer quality and assigns confidence scores."""

import json
import re
from dataclasses import dataclass
from typing import Optional
import ollama

from app.config import AppConfig, get_config


@dataclass
class ValidationResult:
    """Result from the validation agent."""
    confidence_score: int  # 0-100
    reasoning: str
    is_grounded: bool
    is_relevant: bool
    is_complete: bool


VALIDATION_SYSTEM_PROMPT = """You are a quality assurance specialist for a bank customer service system.

Your task is to evaluate answers generated by the system and assign a confidence score based on quality criteria.

Scoring criteria (total 100 points):
1. GROUNDED (0-40 points): Is the answer directly supported by the source content?
   - 40: Fully supported, quotes or closely paraphrases source
   - 25-39: Mostly supported with minor inferences
   - 10-24: Partially supported but includes unsupported claims
   - 0-9: Not well supported by sources

2. RELEVANT (0-30 points): Does the answer address the question asked?
   - 30: Directly and completely addresses the question
   - 20-29: Mostly addresses the question
   - 10-19: Partially addresses the question
   - 0-9: Doesn't address the question well

3. COMPLETE (0-20 points): Does the answer include all necessary information?
   - 20: Complete with all relevant details
   - 15-19: Mostly complete
   - 10-14: Missing some important details
   - 0-9: Significantly incomplete

4. CLEAR (0-10 points): Is the answer clear and actionable?
   - 10: Crystal clear, easy to understand
   - 6-9: Mostly clear
   - 3-5: Somewhat confusing
   - 0-2: Unclear or confusing

You MUST respond in this exact JSON format:
{
    "grounded_score": <0-40>,
    "relevant_score": <0-30>,
    "complete_score": <0-20>,
    "clear_score": <0-10>,
    "is_grounded": <true/false>,
    "is_relevant": <true/false>,
    "is_complete": <true/false>,
    "reasoning": "brief explanation of the scores"
}
"""


class ValidationAgent:
    """Agent that validates answer quality and assigns confidence scores."""

    def __init__(
        self,
        config: Optional[AppConfig] = None,
        ollama_client: Optional[ollama.Client] = None,
    ):
        """Initialize the validation agent.

        Args:
            config: Application configuration
            ollama_client: Ollama client for LLM calls (optional, creates default)
        """
        self.config = config or get_config()
        self.ollama_client = ollama_client or ollama.Client(host=self.config.ollama.host)

    async def validate(
        self,
        question: str,
        answer: str,
        sources: list[str]
    ) -> ValidationResult:
        """Validate an answer and assign a confidence score.

        Args:
            question: The original question
            answer: The generated answer
            sources: List of source passages used to generate the answer

        Returns:
            ValidationResult with confidence score and reasoning
        """
        # Build the prompt with all information
        sources_text = "\n\n---\n\n".join(sources) if sources else "No sources provided"

        user_prompt = f"""Please evaluate the following answer:

QUESTION: {question}

ANSWER: {answer}

SOURCE CONTENT:
{sources_text}

Evaluate the answer quality and provide scores."""

        # Call the LLM
        response = self.ollama_client.chat(
            model=self.config.ollama.llm_model,
            messages=[
                {"role": "system", "content": VALIDATION_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            options={"temperature": 0.2}  # Low temperature for consistent scoring
        )

        response_text = response["message"]["content"]

        # Parse the JSON response
        try:
            json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())

                # Extract scores with defaults
                grounded_score = min(40, max(0, parsed.get("grounded_score", 20)))
                relevant_score = min(30, max(0, parsed.get("relevant_score", 15)))
                complete_score = min(20, max(0, parsed.get("complete_score", 10)))
                clear_score = min(10, max(0, parsed.get("clear_score", 5)))

                confidence_score = grounded_score + relevant_score + complete_score + clear_score

                is_grounded = parsed.get("is_grounded", grounded_score >= 25)
                is_relevant = parsed.get("is_relevant", relevant_score >= 20)
                is_complete = parsed.get("is_complete", complete_score >= 15)
                reasoning = parsed.get("reasoning", "Evaluation completed")
            else:
                # Fallback with moderate scores
                confidence_score = 50
                is_grounded = True
                is_relevant = True
                is_complete = False
                reasoning = "Unable to parse detailed evaluation"
        except json.JSONDecodeError:
            # Fallback on parse error
            confidence_score = 50
            is_grounded = True
            is_relevant = True
            is_complete = False
            reasoning = "Unable to parse detailed evaluation"

        # Ensure confidence score is in valid range
        confidence_score = min(100, max(0, confidence_score))

        return ValidationResult(
            confidence_score=confidence_score,
            reasoning=reasoning,
            is_grounded=is_grounded,
            is_relevant=is_relevant,
            is_complete=is_complete
        )

    def validate_sync(
        self,
        question: str,
        answer: str,
        sources: list[str]
    ) -> ValidationResult:
        """Synchronous version of validate.

        Args:
            question: The original question
            answer: The generated answer
            sources: List of source passages

        Returns:
            ValidationResult with confidence score and reasoning
        """
        import asyncio
        return asyncio.get_event_loop().run_until_complete(
            self.validate(question, answer, sources)
        )
